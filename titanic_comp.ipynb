{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a87fd41-9e84-4210-9336-1c14c5510ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 11), (418, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titanic Competition: https://www.kaggle.com/competitions/titanic/data\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('Dados/titanic/train.csv')\n",
    "test = pd.read_csv('Dados/titanic/test.csv')\n",
    "train = train.set_index('PassengerId')\n",
    "test = test.set_index('PassengerId')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a893eb21-d8b7-478e-b313-a54f0aca7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new feature call 'travel_with_family' and 'size_family', based on last name\n",
    "\n",
    "train['Surname'] = train['Name'].str.split(',').str[0]\n",
    "train['size_family'] = train['Surname'].map(train['Surname'].value_counts())\n",
    "train['travel_with_family'] = train['size_family'] > 1\n",
    "\n",
    "test['Surname'] = test['Name'].str.split(',').str[0]\n",
    "test['size_family'] = test['Surname'].map(test['Surname'].value_counts())\n",
    "test['travel_with_family'] = test['size_family'] > 1\n",
    "\n",
    "map_travel_with_family = {False: 0, True: 1}\n",
    "\n",
    "test['travel_with_family'] = test['travel_with_family'].map(map_travel_with_family)\n",
    "train['travel_with_family'] = train['travel_with_family'].map(map_travel_with_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff190ee-fbb7-4797-bca1-4c71939116a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete the cabin, lets suppose that the family was in same cabin.\n",
    "    \n",
    "nan_cabin_passenger = train.loc[train['Cabin'].isnull() \n",
    "                                & train['travel_with_family'] == 1][\n",
    "                                ['Surname', 'travel_with_family', 'Name', 'Cabin']\n",
    "                                ]\n",
    "cabin_family = train.loc[~train['Cabin'].isnull() & train['Surname'].isin(nan_cabin_passenger['Surname'])][['Name', 'Surname', 'Cabin']]\n",
    "mapping_surname_cabin = cabin_family.drop_duplicates('Surname')\n",
    "mapping_surname_cabin = mapping_surname_cabin.set_index('Surname')['Cabin']\n",
    "train['Cabin'] = train['Cabin'].fillna(train['Surname'].map(mapping_surname_cabin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea005853-9c68-4bad-afdd-bd1b49ae885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with miss values about cabin and encoding the cabin\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# cabins_letter = train['Cabin'].astype(str).str.replace(r'[^a-zA-Z]', '', regex=True).str.split()\n",
    "# def choose_random(val):\n",
    "#     if 'nan' in val or '' in val:\n",
    "#         return 'U'\n",
    "    \n",
    "#     if len(val) > 1:\n",
    "#         choice = np.random.choice(val)\n",
    "#         if len(choice) > 1:\n",
    "#             return choice[0]\n",
    "            \n",
    "#         return choice\n",
    "#     if len(val[0]) > 1:\n",
    "#         return val[0][0]\n",
    "        \n",
    "#     return val[0]\n",
    "\n",
    "# cabins_letter = cabins_letter.apply(choose_random)\n",
    "# train['Cabin'] = cabins_letter\n",
    "\n",
    "# cabins_letter_test = test['Cabin'].astype(str).str.replace(r'[^a-zA-Z]', '', regex=True).str.split()\n",
    "# cabins_letter_test = cabins_letter_test.apply(choose_random)\n",
    "# test['Cabin'] = cabins_letter_test\n",
    "\n",
    "# enc = OrdinalEncoder()\n",
    "# cabin_enc = enc.fit_transform(train[['Cabin']])\n",
    "# train['Cabin'] = cabin_enc[:, 0]\n",
    "\n",
    "# cabin_enc_test = enc.fit_transform(test[['Cabin']])\n",
    "# test['Cabin'] = cabin_enc_test[:, 0]\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['Cabin'] = le.fit_transform(train['Cabin'])\n",
    "test['Cabin'] = le.fit_transform(test['Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080e343a-f6f7-4ce5-8079-57cafc1e057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the missing values about embarked.\n",
    "# I will put the mathematical mode.\n",
    "\n",
    "mode_train = train['Embarked'].value_counts()\n",
    "train['Embarked'] = train['Embarked'].fillna('S')\n",
    "\n",
    "# Encoding \"Embarked\" \n",
    "map_embarked = {'S': 0, 'C': 1, 'Q': 2}\n",
    "train['Embarked'] = train['Embarked'].map(map_embarked)\n",
    "test['Embarked'] = test['Embarked'].map(map_embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe88ff3-99ec-4ef1-8007-b49fbde682c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79776/4184202167.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['Sex'] = train['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n",
      "/tmp/ipykernel_79776/4184202167.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['Sex'] = test['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n"
     ]
    }
   ],
   "source": [
    "train['Sex'] = train['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n",
    "test['Sex'] = test['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fd6503-0d58-4c3f-b581-a858eb723bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'ticket' column contains string data. The authors suggest using an encoding method for this data type. I am applying the Feature Hashing method,\n",
    "#as described in Feature Engineering and Selection by Max Kuhn and Kjell Johnson\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# features = 2**1\n",
    "features = 3\n",
    "\n",
    "h = FeatureHasher(n_features=features, input_type='string')\n",
    "ticket_proc = train['Ticket'].astype(str).str.replace('/', ' ', regex=False).str.replace(r'[^\\w\\s]', '', regex=True).str.split()\n",
    "ticket_feat_hasher = h.fit_transform(ticket_proc).toarray()\n",
    "ticket_feat_hasher_df = pd.DataFrame(ticket_feat_hasher, columns = [f'f{i}' for i in range(len(ticket_feat_hasher[0]))], index = train.index)\n",
    "train = pd.concat([train, ticket_feat_hasher_df], axis=1)\n",
    "train = train.drop(columns = ['Ticket'])\n",
    "\n",
    "\n",
    "ticket_proc = test['Ticket'].astype(str).str.replace('/', ' ', regex=False).str.replace(r'[^\\w\\s]', '', regex=True).str.split()\n",
    "ticket_feat_hasher = h.fit_transform(ticket_proc).toarray()\n",
    "ticket_feat_hasher_df = pd.DataFrame(ticket_feat_hasher, columns = [f'f{i}' for i in range(len(ticket_feat_hasher[0]))], index = test.index)\n",
    "test = pd.concat([test, ticket_feat_hasher_df], axis=1)\n",
    "test = test.drop(columns = ['Ticket'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c42f11-8621-44bd-bb38-3ebf94b1a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age'].isna().sum() / train['Age'].isna().count() # 20% unknow, 80% know. Remove the unknow data is not appropriate\n",
    "\n",
    "# Solution: Impute data with KNN. Source: Feature Engineering and Selection by Max Kuhn and Kjell Johnson\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df_aux = train.select_dtypes(include = ['number'])\n",
    "\n",
    "corr = df_aux.corr()['Age']\n",
    "corr_abs = 0.15\n",
    "corr_age_feat = corr[abs(corr) > corr_abs].index\n",
    "# corr[abs(corr) > corr_abs], corr_age_feat\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "data_to_impute = train[corr_age_feat]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_to_impute)\n",
    "\n",
    "imputer_df_pred = imputer.fit_transform(data_scaled) # KNN\n",
    "imputed_data_final = scaler.inverse_transform(imputer_df_pred) # Return to normal scale\n",
    "\n",
    "df_aux_train = pd.DataFrame(imputed_data_final, columns = corr_age_feat, index=train.index)\n",
    "\n",
    "train['Age'] = df_aux_train['Age']\n",
    "\n",
    "\n",
    "data_to_impute_test = test[corr_age_feat]\n",
    "scaler_test = StandardScaler()\n",
    "data_scaled_test = scaler_test.fit_transform(data_to_impute_test)\n",
    "imputer_df_pred_test = imputer.fit_transform(data_scaled_test) # KNN\n",
    "imputed_data_final_test = scaler_test.inverse_transform(imputer_df_pred_test) # Return to normal scale\n",
    "df_aux_test = pd.DataFrame(imputed_data_final_test, columns = corr_age_feat, index=test.index)\n",
    "test['Age'] = df_aux_test['Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7589943a-2afb-4d92-b6c4-e20d63b552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['Name', 'Surname'])\n",
    "train = train.drop(columns=['Name', 'Surname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d91dc08-6d08-460e-b2e4-7e42535011a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=500)\n",
    "abc = AdaBoostClassifier(n_estimators = 5)\n",
    "gbc = GradientBoostingClassifier(n_estimators = 10)\n",
    "\n",
    "y = train['Survived']\n",
    "train = train.drop(columns=['Survived'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b522fd-5244-4af5-90db-0cd18d5f19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 0.8136902893729208\n"
     ]
    }
   ],
   "source": [
    "rfc_cvs = cross_val_score(rfc, train, y, cv=5)\n",
    "print(f\"Acurácia média: {rfc_cvs.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382c7ca1-c482-4368-80cc-6e3d649811b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 0.7945891657774151\n"
     ]
    }
   ],
   "source": [
    "abc_cvs = cross_val_score(abc, train, y, cv=5)\n",
    "print(f\"Acurácia média: {abc_cvs.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cecc4b2c-4ce3-4aff-8868-5ced06135fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 0.8125729709371665\n"
     ]
    }
   ],
   "source": [
    "gbc_cvs = cross_val_score(gbc, train, y, cv=5)\n",
    "print(f\"Acurácia média: {gbc_cvs.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a79e71-f7a3-4845-ad2f-47bf68a1fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(train)):\n",
    "    X_train_fold, X_test_fold = train.iloc[train_idx], train.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    rfc.fit(X_train_fold, y_train_fold)\n",
    "    score = rfc.score(X_test_fold, y_test_fold)\n",
    "    fold_accuracies.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d67651e1-1aee-4b84-ae2b-274802a5c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bf441b-c1ad-4421-88cf-ac10195495e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test.index,\n",
    "    \"Survived\": y_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f65ae-8e46-4fe6-afbd-7d403f54d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Curso de Dados",
   "language": "python",
   "name": "jupyter_notebook_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
